---
title: "Mini Data-Analysis Deliverable 1"
author: "Eve Chen"
date: "09/10/2021"
output: github_document
---

# Welcome to my first data analysis project!

Let's load the packages below.

```{r, message = FALSE, warning = FALSE}
library(datateachr)
library(tidyverse)
```

# Task 1: Choose your favorite dataset (10 points)

The `datateachr` package by Hayley Boyce and Jordan Bourak currently composed of 7 semi-tidy datasets for educational purposes. Here is a brief description of each dataset:

+ *apt_buildings*: Acquired courtesy of The City of Toronto’s Open Data Portal. It currently has 3455 rows and 37 columns.

+ *building_permits*: Acquired courtesy of The City of Vancouver’s Open Data Portal. It currently has 20680 rows and 14 columns.

+ *cancer_sample*: Acquired courtesy of UCI Machine Learning Repository. It currently has 569 rows and 32 columns.

+ *flow_sample*: Acquired courtesy of The Government of Canada’s Historical Hydrometric Database. It currently has 218 rows and 7 columns.

+ *parking_meters*: Acquired courtesy of The City of Vancouver’s Open Data Portal. It currently has 10032 rows and 22 columns.

+ *steam_games*: Acquired courtesy of Kaggle. It currently has 40833 rows and 21 columns.

+ *vancouver_trees*: Acquired courtesy of The City of Vancouver’s Open Data Portal. It currently has 146611 rows and 20 columns.

**1.1** Out of the 7 datasets available in the `datateachr` package, choose **4** that appeal to you based on their description. Write your choices below:

1: *apt_buildings*    
2: *cancer_sample*    
3: *flow_sample*    
4: *steam_games*    

**1.2** One way to narrowing down your selection is to *explore* the datasets. Use your knowledge of dplyr to find out at least *3* attributes about each of these datasets (an attribute is something such as number of rows, variables, class type...). The goal here is to have an idea of *what the data looks like*. 

*Hint:* This is one of those times when you should think about the cleanliness of your analysis. I added a single code chunk for you, but do you want to use more than one? Would you like to write more comments outside of the code chunk?

**Exploring dataset 1: *apt_buildings***

```{r}
### Class of the dataset ###
class(apt_buildings)
### Get a glimpse of the data ###
glimpse(apt_buildings)
```

**Attributes about *apt_buildings* include:**

- Class of the dataset: `c("tbl_df", "tbl", "data.frame")`, which means it is a tibble that has been coerced from a dataframe. 
- Number of rows: 3455
- Number of columns: 37
- Types of data in columns: 9 numeric types, 28 character types.
- Columns names
- Contents of variables in each column

**Comments for *apt_buildings* :**

- This dataset consists of different building information of 3455 apartments, including the year they were built and registered, number of units, facility conditions and so on. 
- Some columns that are `character` type only consist of 2 factors as "YES" or "NO". 
- It's possible that we can use it to study the relationship between the age of the buildings and the characteristics.

**Exploring dataset 2: *cancer_sample***

```{r}
### Class of the dataset ###
class(cancer_sample)
### Get a glimpse of the data ###
glimpse(cancer_sample)
```

**Attributes about *cancer_sample* include:**

- Class of the dataset: `c("spec_tbl_df", tbl_df", "tbl", "data.frame")`, as a tibble subclass. 
- Number of rows: 569
- Number of columns: 32
- Types of data in columns: 31 numeric types, 1 character types (`diagnosis`).
- Columns names
- Contents of variables in each column

**Comments for *cancer_sample*:**

- The `spec_tbl_df` subclass is new and special to me. According to a documentation, it "differs from a regular tibble only in that the `spec` attribute (which holds the column specification) is lost as soon as the object is subset and a normal `tbl_df` object is returned." 
- This dataset includes 569 observations of diagnostic cancer data. 
- Besides the ID of samples and the ultimate diagnosis of the breast mass ("M" = malignant, "B" = benign), the columns are mostly **quantitative features** derived from the sample image. 
- Both statistic and clinical knowledge might be required to understand the variables and decide the ways to analyze and interpret the data.

**Exploring dataset 3: *flow_sample***

```{r}
### Class of the dataset ###
class(flow_sample)
### Get a glimpse of the data ###
glimpse(flow_sample)
```
**Attributes about *flow_sample* include:**

- Class of the dataset: `c("tbl_df", "tbl", "data.frame")`, which means it is a tibble that has been coerced from a dataframe. 
- Number of rows: 218
- Number of columns: 7
- Types of data in columns: 4 numeric types, 3 character types.
- Columns names
- Contents of variables in each column

**Comments for *flow_sample*:**

- This dataset includes 218 observations of flow rate extrema data and additional information regarding the date and conditions of data collection. 
- If we see the `extreme_type` and the flow rate `flow` as dependent variable and the others as possible factors, we can have multiple angles to study the relationship between them by correlations. 

**Exploring dataset 4: *steam_games***

```{r}
### Class of the dataset ###
class(steam_games)
### Get a glimpse of the data ###
glimpse(steam_games)
```

**Attributes about *steam_games*:**

- Class of the dataset: `c("spec_tbl_df", tbl_df", "tbl", "data.frame")`, as a tibble subclass.
- Number of rows: 40833
- Number of columns: 21
- Types of data in columns: 4 numeric types, 17 character types.
- Columns names
- Contents of variables in each column

**Comments for *steam_games*:**

- It has 40833 observations, which is the most out of the 4 datasets that I've chosen.
- The columns of this set contains free text information as the url, names, developer and other details of the games. There's also common features across them that can be summarized, such as `types` and `genre`. There are also numeric features like `achivements` and `price` that seem interesting.  

**1.3** Now that you've explored the 4 datasets that you were initially most interested in, let's narrow it down to 2. What lead you to choose these 2? Briefly explain your choices below, and feel free to include any code in your explanation. 

**My two choices and explanation:**

+ The *flow_sample* dataset:

The variables in each column of this dataset is well organized (formats are uniformed for each observation) and easy to understand. I thought of some questions that can possibly be answered by analyzing these data at first glance. I think it would be interesting for me to learn about the fluctuation of minimum and maximum flow rate and how that changes from year to year and if our treatment of the environment and global warming has changed this throughout the years.

+ The *steam_games* dataset:

First of all, I like Steam and this dataset looks fun. And as I mentioned above I was interested by the class of this dataset `spec_tbl_df`, as we didn't seem to cover this in class. Even though the information in many columns looks kind of messy and hard to deal with, I would still want to study the relationship between the reviews, achievements and the price difference of the games. 

**1.4** Time for the final decision! Going back to the beginning, it's important to have an *end goal* in mind. For example, if I had chosen the `titanic` dataset for my project, I might've wanted to explore the relationship between survival and other variables. Try to think of 1 research question that you would want to answer with each dataset. Note them down below, and make your final choice based on what seems more interesting to you!

**Research questions for each dataset:**

1. *apt_buildings*: I want to study the distribution of types of heating systems (`heating_type`) for buildings built in different year (`year_built`). I was assuming that the older buildings are using hot water and the newer ones tend to use electric or forced air gas systems.

2. *cancer_sample*: I would be interested in the general range of the data like `radius_mean`, `texture_mean` and `area mean` where the sample can be diagnosed accordingly as benign `B` or malignant `M`.

3. *flow_sample*: I want to explore the relationship between extremum flow rate (`flow`) and other variables of time and conditions. For example, I want to see if there are year by year long term changes of the extremum flow rate, or if there are days in a month or months in a year when maximum or minimum flow rates (`extreme_type`) commonly occur.

4. *steam_games*: I want to study the common differences (extreme value or portion) between `original_price` and `discount_price` so it would be convenient to make a prediction in the future. Also we can study the relationship between this differences and the `achievements` that the games made. 

**My final choice: *flow_sample***

They all look interesting to me, but based on the simple inspection of the datasets, I'm more confident in *flow_sample* since its data looks more organized and clean - less `NA` values and less variables that are hard to be factored or quantified.

# Task 2: Exploring your dataset (15 points)

If we rewind and go back to the learning objectives, you'll see that by the end of this deliverable, you should have formulated *4* research questions about your data that you may want to answer during your project. However, it may be handy to do some more exploration on your dataset of choice before creating these questions - by looking at the data, you may get more ideas. **Before you start this task, read all instructions carefully until you reach START HERE**.

2.1 Complete *4 out of the following 8 exercises* to dive deeper into your data. All datasets are different and therefore, not all of these tasks may make sense for your data - which is why you should only answer *4*. Use *dplyr* and *ggplot*.

1. **Plot the distribution of a numeric variable.**
2. Create a new variable based on other variables in your data (only if it makes sense)
3. Investigate how many missing values there are per variable. Can you find a way to plot this?
4. **Explore the relationship between 2 variables in a plot.**
5. **Filter observations in your data according to your own criteria.** Think of what you'd like to explore - again, if this was the `titanic` dataset, I may want to narrow my search down to passengers born in a particular year...
6. **Use a boxplot to look at the frequency of different observations within a single variable. You can do this for more than one variable if you wish!**
7. Make a new tibble with a subset of your data, with variables and observations that you are interested in exploring.
8. Use a density plot to explore any of your variables (that are suitable for this type of plot).

2.2 For each of the 4 exercises that you complete, provide a *brief explanation* of why you chose that exercise in relation to your data (in other words, why does it make sense to do that?), and sufficient comments for a reader to understand your reasoning and code.

# Task 3: Write your research questions (5 points)

So far, you have chosen a dataset and gotten familiar with it through exploring the data. Now it's time to figure out 4 research questions that you would like to answer with your data! Write the 4 questions and any additional comments at the end of this deliverable. These questions are not necessarily set in stone - TAs will review them and give you feedback; therefore, you may choose to pursue them as they are for the rest of the project, or make modifications!

<!--- *****START HERE***** --->

# Task 2: Exploring *flow_sample*:

#### 1. Plot the distribution of a numeric variable.

First, I want to explore the distribution of the flow rate (`flow`). This is one of the most informative variable from this dataset so knowing its range and distribution would help with the following tests.

```{r}
### Make a histogram of flow, have the height of the bars represent the density, and distinguish extreme_type by colour
flow_sample %>% 
  ggplot(aes(x = flow, y = ..density..)) +
   geom_histogram(aes(fill = extreme_type), bins = 10, na.rm = T) +
   labs(x = "Extremum flow rate in m^3/s.")
```

From this histogram, we can see that:

- Most of the extremum flow rate value recorded is around `0 m^3/s`, and they are all records for the minimum flow of each year. 
- For the maximum record, bars with the most frequencies are between flow rate `150 - 350 m^3/s`. There are very few cases where the flow rate is around `450 m^3s`, the highest of all. 

#### 4. Explore the relationship between 2 variables in a plot.

After the first histogram, I really want to know if they record the minimum and maximum rate in certain time. I would use ` jitterplot` to explore how `flow` rate is spread in each `month`.

```{r}
### Make a jitter plot of flow and month, and distinguish extreme_type by colour
flow_sample %>% 
  ggplot(aes(as.factor(month), flow, color = extreme_type)) + 
    geom_jitter(alpha = 0.5, width = 0.1, na.rm = T)
```

- From this plot, it's obvious that the maximum flow is usually collected between May to July. And the minimum flow can be collected in other months.
- The highest extreme flow rate is collected in June.

#### 5. Filter observations in your data according to your own criteria.

Now I want to investigate further on specifically what day in a month would the `maximum` rate usually occur and be recorded. For this, I want to narrow my search down to records of `maximum` flow and all the corresponding information.

```{r}
### Create a subset that only looks at observations when maximum flow per year was recorded
flow_sample_max <-
  flow_sample %>% 
  filter(extreme_type == "maximum")
### Take a look at the new subset
head(flow_sample_max)
### Check the size of the new subset
dim(flow_sample_max)
```

- The subset is narrowed down to `109` observations, which is half the size of the original dataset.

#### 6. Use a boxplot to look at the frequency of different observations within a single variable.

Better make use of the new subset now! Boxplots are a nice approach for us to visualize distributions of the `flow` rate and the `day` it was recorded. 

```{r}
### Make a boxplot of rate and day, separately observe them by month.
flow_sample_max %>%
  ggplot(aes(as.factor(day), flow)) + 
    geom_boxplot(width = 0.5) +
    facet_wrap(~ as.factor(month))
```

- I know they are not the most beautiful boxplots to be observed. But from this plot we can tell that most of the data was recorded during late May, early July and the whole June. 
- Range of the `flow` are diverse enough for June, so now we might safely move forward to focus on the days in this month.

```{r}
### Make a boxplot of rate and day in June.
flow_sample_max %>%
  filter(month == 6) %>%
  ggplot(aes(as.factor(day), flow)) + 
    geom_boxplot(width = 0.5) 
```

- We can now see that the flow rate over `300 m^3/s` usually occurs mid June (14-21). 
- On June 21 the flow rate recorded is most likely to be the highest, spreading around `380 m^3/s.

# Task 3: My research questions for *flow_sample*

1. Which year had the greatest difference in maximum and minimum flow rate? 
1. Is there a trend (increases or decreases) of the difference between `maximum` and `minimum` flow rate over time (in `years`)?
1. If there is a trend, is it related to the `day` or `month` when the flow rate is recorded? - Could it because they have recorded some maximum flow rate on days when it was not really maximum?
1. There are 4 kinds of variables in column `sym`: *A: partial day, B: ice condtions, E: estimated, S: sample(s) collected this day, NA: no additional info*. How is the flow rate recorded possibly affected by these additional info?

### Attribution

Thanks to Icíar Fernández Boyano for mostly putting this together, and Vincenzo Coia for launching.
