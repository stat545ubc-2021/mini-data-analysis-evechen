---
title: "Mini Data Analysis Milestone 2"
author: "Eve Chen"
date: "10/12/2021"
output: github_document
---

# Welcome back to my mini data analysis project!

This time, we will explore more in depth the concept of *tidy data*, and hopefully investigate further into your research questions that you defined in milestone 1.

**NOTE**: The main purpose of the mini data analysis is to integrate what you learn in class in an analysis. Although each milestone provides a framework for you to conduct your analysis, it's possible that you might find the instructions too rigid for your data set. If this is the case, you may deviate from the instructions -- just make sure you're demonstrating a wide range of tools and techniques taught in this class.

Begin by loading your data and the tidyverse package below: 

```{r, message = FALSE, warning = FALSE}
library(datateachr) # <- might contain the data you picked!
library(tidyverse)
```

# Learning Objectives

By the end of this milestone, you should:

+ Become familiar with manipulating and summarizing your data in tibbles using `dplyr` and `tidyr`, with a research question in mind.
+ Understand what *tidy* data is, and how to create it. In milestone 3, we will explore when this might be useful.
+ Generate a reproducible and clear report using R Markdown.
+ Gain a greater understanding of how to use R to answer research questions about your data.

**Things to keep in mind**

+ Remember to document your code, be explicit about what you are doing, and write notes in this markdown document when you feel that context is required. Create your analysis as if someone else will be reading it! **There will be 2.5 points reserved for reproducibility, readability, and repo organization.**

+ Before working on each task, you should always keep in mind the specific **research question** that you're trying to answer.

# Task 1: Process and summarize your data (15 points)

From milestone 1, you should have an idea of the basic structure of your dataset (e.g. number of rows and columns, class types, etc.). Here, we will start investigating your data more in-depth using various data manipulation functions. 

### 1.1 (2.5 points) 

First, write out the 4 research questions you defined in milestone 1 were. This will guide your work through milestone 2:

<!-------------------------- Start your work below ---------------------------->
**My dataset: *flow_sample***

1. Is there a trend (increases or decreases) of the `maximum` and `minimum` flow rate over time (in `years`)?
2. How is the `maximum` and `minimum` flow rate records related to the month of the year?
3. Is there *seasonality* to when `maximum` and `minimum` flow rates are usually recorded?
4. There are 4 kinds of variables in column `sym`: *A: partial day, B: ice condtions, E: estimated, S: sample(s) collected this day, NA: no additional info*. How is the `flow` rate recorded possibly affected by these additional info?
<!----------------------------------------------------------------------------->

### 1.2 (10 points)

Now, for each of your four research questions, choose one task from options 1-4 (summarizing), and one other task from 4-8 (graphing). You should have 2 tasks done for each research question (8 total). Make sure it makes sense to do them! (e.g. don't use a numerical variables for a task that needs a categorical variable.). Comment on why each task helps (or doesn't!) answer the corresponding research question. 

Ensure that the output of each operation is printed! 

**Summarizing:**

1. Compute the *range*, *mean*, and *two other summary statistics* of **one numerical variable** across the groups of **one categorical variable** from your data.
2. Compute the number of observations for at least one of your categorical variables. Do not use the function `table()`!
3. Create a categorical variable with 3 or more groups from an existing numerical variable. You can use this new variable in the other tasks! *An example: age in years into "child, teen, adult, senior".*
4. Based on two categorical variables, calculate two summary statistics of your choosing.

**Graphing:**

5. Create a graph out of summarized variables that has at least two geom layers.
6. Create a graph of your choosing, make one of the axes logarithmic, and format the axes labels so that they are "pretty" or easier to read. 
7. Make a graph where it makes sense to customize the alpha transparency.
8. Create 3 histograms out of summarized variables, with each histogram having different sized bins. Pick the "best" one and explain why it is the best.

Make sure it's clear what research question you are doing each operation for!

<!------------------------- Start your work below ----------------------------->
#### 1.2 Operations

**1. Is there a trend (increases or decreases) of the `maximum` and `minimum` flow rate over time (in `years`)?**

- First, let's filter all observasions with no flow records, and convert all character columns in `flow_sample` to factors 

```{r convert_factor}
flow_sample_factor <- flow_sample %>%
  drop_na("flow") %>%
  mutate_if(sapply(flow_sample, is.character), as.factor)
head(flow_sample_factor)
```

- (*Summarizing 1*) Now, we can compute the *range* (*max* and *min*) , *mean*,  of `flow` across the groups of `extream_type` from your data to observe the overall range and statistical features of the flow rate recorded as different type.

```{r }
flow_sample_factor %>%
  group_by(extreme_type) %>% 
  summarise(flow_maximum = max(flow),
            flow_minimum = min(flow),
            flow_range = max(flow) - min(flow),
            flow_mean = mean(flow),
            flow_median = median(flow),
            n = n())
```

- Based on the statistics above, we have a clearer idea of what `extreme_type` indicates - `maximum` for biggest flow rate recorded that year, and vise versa.

- (*Graphing 5*) Now we can simply take a look at the flow rate trend of each year based on different type - create a graph out of `flow` that has 2 layers: `geom_line` and `geome_point`

```{r }
flow_sample%>% 
  group_by(extreme_type) %>% 
  ggplot(aes(year, flow, colour = extreme_type)) +
  geom_point(size = 1, alpha = 0.8) +
  geom_line() +
  labs(x = "Year", y = "Flow Rate in m^3/s", color = "Extreme type") +
  theme_bw() 
```

- From the graph above, we see that `flow` of each type is **fluctuating** within the range we summarized before, and there **isn't** an obvious trend of the `maximum` and `minimum` flow rate over `year`.  

**2. How is the `maximum` and `minimum` flow rate records related to the month of the year?**

- (*Summarizing 2*) I wanna compute the number of observations for `extreme_type` and `month`, which can be treated as categorical variables.  

```{r }
# How many records are there for each month:
flow_sample_factor %>%
  group_by(factor(month), extreme_type) %>% 
  count()
```

- There are 0 records of September and October, and only 1 maximum record from August. Maximums are all recorded from May to August while Minimums are recorded from November to April.

- (*Graphing 7*) Make a graph where it makes sense to customize the alpha transparency. - I wanna how has the distribution of `flow` changed over `months` for each type of flow rate records (`extreme_type`)?

```{r }
# I plot the ridges of flow rate over months, adjust the alpha transparency in case there will be overlapping ridges.
flow_sample_factor %>% 
  ggplot(aes(flow, factor(month))) +
  ggridges::geom_density_ridges(aes(fill = extreme_type), alpha = 1/3) +
  labs(x = "Flow Rate in m^3/s", y = "Month", fill = "Extreme type")+
  theme_minimal()
```

**3. Is there seasonality to when maximum and minimum flow rates are usually recorded?**

- (*Summarizing 3*) Create a categorical variable `season` with 4 groups from an existing numerical variable `month`. 

```{r}
# Create a column of seasons with 4 levels from months. 
flow_sample_season <- flow_sample_factor %>% 
  mutate(season = factor(case_when(month %in% c(3, 4, 5) ~ "Spring",
                                   month %in% c(6, 7, 8) ~ "Summer",
                                   month %in% c(9, 10, 11) ~ "Autumn",
                                   month %in% c(12, 1, 2) ~ "Winter"),
                         levels = c("Spring", "Summer", "Autumn", "Winter"))) 
flow_sample_season %>%
  ggplot()+
  geom_boxplot(aes(x = season, y = flow, color = extreme_type)) +
  labs(y = "Flow Rate in m^3/s", x= "Seasons", color = "Extreme Type") +
  theme_bw() 
```

- (*Grapphing 4*) Create 3 histograms out of summarized variables, with each histogram having different sized bins. Pick the "best" one and explain why it is the best.

I wanna see how the `flow` rate collected is distributed by seasons. 

```{r}
# We can first take a look at the range of the flow rate. 
range(flow_sample_season$flow)

# The difference is around 0 - 500, so we can divide the bins accordingly:
# 50 bins, width around 10 each
flow_sample_season %>%
  ggplot(aes(x = flow, fill = season)) +
  geom_histogram(bins = 50, alpha = 0.8) +
  labs(x = "Flow rate in m^3/s", y = "Flow rate times recorded", fill = "Seasons")+
  theme_bw() 
# 10 bins, width around 50 each
flow_sample_season %>%
  ggplot(aes(x = flow, fill = season)) +
  geom_histogram(bins = 10, alpha = 0.8) +
  labs(x = "Flow rate in m^3", y = "Flow rate times recorded", fill = "Seasons")+
  theme_bw() 
# 5 bins, width around 100 each
flow_sample_season %>%
  ggplot(aes(x = flow, fill = season)) +
  geom_histogram(bins = 5, alpha = 0.8) +
  labs(x = "Flow rate in m^3", y = "Flow rate times recorded", fill = "Seasons")+
  theme_bw() 
```

- The best bin size I choose is `10`. The bins are well separated but not too scattered. I can observe an overall range where the `flow` rate collected in certain `seasons` will be distributed. 


**4. There are 4 kinds of variables in column `sym`: *A: partial day, B: ice condtions, E: estimated, S: sample(s) collected this day, NA: no additional info*. How is the `flow` rate recorded possibly affected by these additional info?**

- (*Summarizing 4*) Based on two categorical variables, calculate two summary statistics of your choosing.

```{r}
flow_sample_season %>%
  group_by(extreme_type, sym) %>%
  summarise(flow_max = max(flow),
            flow_min = min(flow),
            flow_mean = mean(flow),
            records = n())
```

- There are only 2 records where the flow rate is `E:estimated`, one for each extreme type. `A: partial day` is only with the `maximum` records, while `B: ice conditions` only happens when `minimum` is recorded. 

- (*Graphing 3*) I wanna know how `B:ice conditions` affect the minimum rate recorded. Create a graph of your choosing, make one of the axes logarithmic, and format the axes labels so that they are "pretty" or easier to read. 

```{r}
flow_sample_season  %>% 
  filter(extreme_type == "minimum")  %>% 
  ggplot(aes(flow, year, colour = sym)) +
  geom_point(size = 1, alpha = 0.8) +
  scale_x_log10(labels = scales::label_number(suffix = "m^3/s")) +
  scale_color_discrete(labels = c('B: Ice condtions','E: Estimated','NA: No additional info')) +
  labs(x = "Flow Rate", y = "Year", colour = "Record infomation") +
  facet_wrap(~ sym, ncol = 2) +
  theme_bw() 
```

- We can see that mostly when `minimum flow` is recorded, it's of `B: ice conditions` and the values recorded at `B` can ba both lowest or highest among all. So the record information `sym` doesn't appear to have an obvious effect on the `minimum flow`

<!----------------------------------------------------------------------------->

### 1.3 (2.5 points)

Based on the operations that you've completed, how much closer are you to answering your research questions? Think about what aspects of your research questions remain unclear. Can your research questions be refined, now that you've investigated your data a bit more? Which research questions are yielding interesting results?

<!------------------------- Write your answer here ---------------------------->

- I believe I am quite close in answering all of the questions. The first question on whether there are trends of `maximum/minimum flow` in `year` still seems unclear at this point. It may be useful to split the years up into further periods and to assess for variability in the data as well as looking further at the range of maximum and minimum flow rates. 

- It may be useful to further study the `flow` trends by refining the data with the observations given of both `year` and `sym`, to see if the special conditions in certain year may have affected the `extremum flow rate`.

- The relationship between the `seasons` when `maximum` and `minimum` flow rate are recorded is interesting. it appears `maximum` flow rate is almost always recorded during `summer` months and sometimes `spring`, whereas the majority of `minimum` flow rate happens in the `winter` and some in the `spring`, few in the `autumn`.

<!----------------------------------------------------------------------------->

# Task 2: Tidy your data (12.5 points)

In this task, we will do several exercises to reshape our data. The goal here is to understand how to do this reshaping with the `tidyr` package.

A reminder of the definition of *tidy* data:

- Each row is an **observation**
- Each column is a **variable**
- Each cell is a **value**

*Tidy'ing* data is sometimes necessary because it can simplify computation. Other times it can be nice to organize data so that it can be easier to understand when read manually. 

### 2.1 (2.5 points)

Based on the definition above, can you identify if your data is tidy or untidy? Go through all your columns, or if you have >8 variables, just pick 8, and explain whether the data is untidy or tidy.

<!--------------------------- Start your work below --------------------------->
```{r}
# Take another look of the data
glimpse(flow_sample)
```
- Looking at each of the variables, it seems that the data is tidy. 
  - Each row is an **observation**: Each row is a unique record with different `flow` rate, type and recorded data. The `station_id` is the unique ID of the hydrometric station where measurements of flow is made. The recorded data is split up into three variables of the `year`, `month` and `day`. And additional information `sym` also corresponds uniquely to the observation itselt.
  - Each column is a **variable** - not **value**.
  - Each cell is a **value** - some are `NA` values.
  
- However, if we only wanna know the extremeum data of **each year**, ignoring more information as `month`, `day` and `sym` that actually differs for what time in a year when 2 types of data was collected, we can further tidy the data to half the size. 

<!----------------------------------------------------------------------------->

### 2.2 (5 points)

Now, if your data is tidy, untidy it! Then, tidy it back to it's original state.

If your data is untidy, then tidy it! Then, untidy it back to it's original state.

Be sure to explain your reasoning for this task. Show us the "before" and "after".

<!--------------------------- Start your work below --------------------------->

- Untidy *flow_sample*: Using `values` in `extreme_type` as `variables` of the `flow` column.

```{r}
# To untidy it, Use `extreme_type` as names of the `flow` column.
(flow_untidy <- 
  flow_sample %>% 
  pivot_wider(id_cols = c(-extreme_type, -flow), 
                names_from = extreme_type,
                values_from = flow))
```

- We can tidy this by putting `maximum` and `minimum` back into the same column variable: `extreme_type`.

```{r}
(flow_tidy <-
  flow_untidy %>% 
  pivot_longer(cols = c(maximum, minimum), 
               names_to = "extreme_type",
               values_to = "flow",
               values_drop_na = T)) 
   # dropping replicated observations with empty `flow` values would also drop some observations that had `NA` recorded in the original dataset.
```

<!----------------------------------------------------------------------------->

### 2.3 (5 points)

Now, you should be more familiar with your data, and also have made progress in answering your research questions. Based on your interest, and your analyses, pick 2 of the 4 research questions to continue your analysis in milestone 3, and explain your decision. 

Try to choose a version of your data that you think will be appropriate to answer these 2 questions in milestone 3. Use between 4 and 8 functions that we've covered so far (i.e. by filtering, cleaning, tidy'ing, dropping irrelvant columns, etc.). 

<!--------------------------- Start your work below --------------------------->

- I have chosen: 
  - 1. Has maximum and minimum flow rate changed over different time periods (beyond `years`)? 
  
  Both of these questions have yielded interesting results in terms of showing trends in the data. It would be interesting to investigate further why 2000s has greater variability (may be due to other factors like ice conditions), as well as looking further into seasonality and when maximum/minimum flow rate is recorded.
  
  - 2. Is there seasonality (together with other factors) to when maximum and minimum flow rates are usually recorded? 
  
  Both of these questions have yielded interesting results in terms of showing trends in the data. It would be interesting to investigate further why 2000s has greater variability (may be due to other factors like ice conditions), as well as looking further into seasonality and when maximum/minimum flow rate is recorded.

<!----------------------------------------------------------------------------->

*When you are done, knit an `md` file. This is what we will mark! Make sure to open it and check that everything has knitted correctly before submitting your tagged release.*

### Attribution

Thanks to Victor Yuan for mostly putting this together. 